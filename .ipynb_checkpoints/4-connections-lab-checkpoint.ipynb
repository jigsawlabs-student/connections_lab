{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "under-watson",
   "metadata": {},
   "source": [
    "# Connections Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-listening",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-taylor",
   "metadata": {},
   "source": [
    "In this lesson, we'll practice using airflow to connect to our AWS services.  Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-calculation",
   "metadata": {},
   "source": [
    "### Connecting to RDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-wichita",
   "metadata": {},
   "source": [
    "Now for this task, let's create a new dag that performs two tasks:\n",
    "\n",
    "1. Create a task called `five_venues` that queries and logs the first five venues\n",
    "2. Create a task called `three_categories` that queries and log the first three categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-steal",
   "metadata": {},
   "source": [
    "To accomplish this, we can connect to an RDS instance already set up.  Here is some information you'll need to connect to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "executive-sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "host = \"foursquare-flask-api.cbdkozm37vkd.us-east-1.rds.amazonaws.com\"\n",
    "database = 'postgres'\n",
    "password = 'password1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-guess",
   "metadata": {},
   "source": [
    "Ok, now let's create the dag and the two tasks, and use a PostgresHook to produce the requested information in the task logs.  If things are working properly, we can begin by seeing that the tasks have successfully performed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-testament",
   "metadata": {},
   "source": [
    "<img src=\"./perform_queries.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-routine",
   "metadata": {},
   "source": [
    "And then we can check the logs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-toronto",
   "metadata": {},
   "source": [
    "> So first we can see the values from `get_venues` returned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-console",
   "metadata": {},
   "source": [
    "<img src=\"./get_venues.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-miller",
   "metadata": {},
   "source": [
    "> And then in the `get_categories` task, we can see that the values of Pizza, Italian, and Bar were returned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-brazilian",
   "metadata": {},
   "source": [
    "<img src=\"./get_categories.png\" width=\"90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-webcam",
   "metadata": {},
   "source": [
    "Great, so we were successfully able to connect to our instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-crime",
   "metadata": {},
   "source": [
    "### Connecting to Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-portal",
   "metadata": {},
   "source": [
    "Now let's try to connect to airflow to a redshift cluster.  And from there, we can even attempt to copy over some data from S3 over to redshift.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-eleven",
   "metadata": {},
   "source": [
    "1. Create the redshift cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-fishing",
   "metadata": {},
   "source": [
    "To begin, create the redshift cluster by going [here](https://console.aws.amazon.com/redshift/home).  And then setup the cluster with the security group giving public access, and attaching an IAM role that gives read only access to S3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-extreme",
   "metadata": {},
   "source": [
    "2. Connect to the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-confidence",
   "metadata": {},
   "source": [
    "Then, before connecting to the cluster in airflow, let's make sure that we set everything up properly by connecting to the cluster with the `pyscopg2` library.  \n",
    "\n",
    "Fill in the proper, information and create the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "funny-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "endpoint = \"redshift-2.cdpgnoufdsdf.us-east-1.redshift.amazonaws.com\"\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=endpoint,\n",
    "    database=\"dev\",\n",
    "    port = \"5439\",\n",
    "    user=\"awsuser\",\n",
    "    password=\"Password1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "romance-challenge",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-exchange",
   "metadata": {},
   "source": [
    "3. Create an initial table\n",
    "\n",
    "Then we'll need to create our tables in redshift.  For this lesson, we'll just practice copying over the zipcodes table, so let's create that table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "metric-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_zipcodes_query = \"\"\"CREATE TABLE \"zipcodes\" (\n",
    "    \"id\" integer NOT NULL DEFAULT nextval('zipcodes_id_seq'),\n",
    "    \"code\" INTEGER,\n",
    "    \"city_id\" INTEGER\n",
    ");\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cordless-canvas",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(create_zipcodes_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "lucky-bleeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-butler",
   "metadata": {},
   "source": [
    "4. Connect to redshift in airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-booking",
   "metadata": {},
   "source": [
    "Ok, now it's time to connect to redshift in airflow.  To do so, we'll need to: \n",
    "\n",
    "* Create a connection in airflow\n",
    "* Create a task that copies data from airflow\n",
    "    * To create the task, we can use the `connection.run` function, and the query below.\n",
    "    > Fill in the s3 bucket name, and the `aws_iam_role` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-ambassador",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = \"\"\"COPY zipcodes (id, code, city_id) from 's3://jigsaw-sample-data/zipcodes.csv'\n",
    "credentials 'aws_iam_role=arn:aws:iam::095598444804:role/myRedshiftRole'\n",
    "delimiter ','\n",
    "IGNOREHEADER 1\n",
    "region 'us-east-1';\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-detector",
   "metadata": {},
   "source": [
    "* Then after creating a task that runs the copy command, create another task that selects from redshift's zipcodes table to confirm the zipcodes were copied over."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-signature",
   "metadata": {},
   "source": [
    "So in summary, we should now have a dag with three tasks: the first queries our rds, the second copies over data from s3 to redshift, and the third queries the data in redshift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-homeless",
   "metadata": {},
   "source": [
    "So if we look at a tree view of our DAG, we should see the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-summit",
   "metadata": {},
   "source": [
    "> <img src=\"./airflow_dag.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-tension",
   "metadata": {},
   "source": [
    "And if we ultimately view the log of the `select_zipcodes` task, we should see our zipcodes loaded in there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-breathing",
   "metadata": {},
   "source": [
    "> <img src=\"./returned_zipcodes.png\" width=\"90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-compromise",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-auction",
   "metadata": {},
   "source": [
    "In this lesson, we practiced working with and setting up connections so that we could connect to our RDS database.  In future lessons, we'll see how we can use the connections to perform our steps of loading data RDS to S3 and S3 to redshift."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
